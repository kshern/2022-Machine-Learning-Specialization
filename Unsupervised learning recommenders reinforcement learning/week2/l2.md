# L2
So let's take a look at how we can develop a recommended system if we had features of each item, or features of each movie. So here's the same data set that we had previously with the four users having rated some but not all of the five movies. What if we additionally have features of the movies? So here I've added two features X1 and X2, that tell us how much each of these is a romance movie, and how much each of these is an action movie. So for example Love at Last is a very romantic movie, so this feature takes on 0.9, but it's not at all an action movie. So this feature takes on 0. But it turns out Nonstop Car chases has just a little bit of romance in it. So it's 0.1, but it has a ton of action. So that feature takes on the value of 1.0. 

So you recall that I had used the notation nu to denote the number of users, which is 4 and m to denote the number of movies which is 5. I'm going to also introduce n to denote the number of features we have here. And so n=2, because we have two features X1 and X2 for each movie. With these features we have for example that the features for movie one, that is the movie Love at Last, would be 0.90. And the features for the third movie Cute Puppies of Love would be 0.99 and 0. And let's start by taking a look at how we might make predictions for Alice's movie ratings. So for user one, that is Alice, let's say we predict the rating for movie i as w.X(i)+b. So this is just a lot like linear regression. For example if we end up choosing the parameter w(1)=[5,0] and say b(1)=0, then the prediction for movie three where the features are 0.99 and 0, which is just copied from here, first feature 0.99, second feature 0. Our prediction would be w.X(3)+b=0.99 times 5 plus 0 times zero, which turns out to be equal to 4.95. And this rating seems pretty plausible. 

It looks like Alice has given high ratings to Love at Last and Romance Forever, to two highly romantic movies, but given low ratings to the action movies, Nonstop Car Chases and Swords vs Karate. So if we look at Cute Puppies of Love, well predicting that she might rate that 4.95 seems quite plausible. And so these parameters w and b for Alice seems like a reasonable model for predicting her movie ratings. Just add a little the notation because we have not just one user but multiple users, or really nu equals 4 users. I'm going to add a superscript 1 here to denote that this is the parameter w(1) for user 1 and add a super strip 1 there as well. And similarly here and here as well, so that we would actually have different parameters for each of the 4 users on data set. And more generally in this model we can for user j, not just user 1 now, we can predict user j's rating for movie i as w(j).X(i)+b(j). So here the parameters w(j) and b(j) are the parameters used to predict user j's rating for movie i which is a function of X(i), which is the features of movie i. And this is a lot like linear regression, except that we're fitting a different linear regression model for each of the 4 users in the dataset. So let's take a look at how we can formulate the cost function for this algorithm. As a reminder on notation is that r(i.,j)=1 if user j has rated movie i or 0 otherwise. And y(i,j)=rating given by user j on movie i. 

And on the previous side we defined w(j), b(j) as the parameters for user j. And X(i) as the feature vector for movie i. So the model we have is for user j and movie i predict the rating to be w(j).X(i)+b(j). I'm going to introduce just one new piece of notation, which is I'm going to use m(j) to denote the number of movies rated by user j. So if the user has rated 4 movies, then m(j) would be equal to 4. And if the user has rated 3 movies then m(j) would be equal to 3. So what we'd like to do is to learn the parameters w(j) and b(j), given the data that we have. That is given the ratings a user has given of a set of movies. So the average we're going to use is very similar to linear regression. So let's write out the cost function for learning the parameters w(j) and b(j) for a given user j. And let's just focus on one user on user j for now. I'm going to use the mean squared error criteria. So the cost will be the prediction, which is w(j).X(i)+b(j) minus the actual rating that the user had given. So minus y(i,j) squared. And we're trying to choose parameters w and b to minimize the squared error between the predicted rating and the actual rating that was observed. But the user hasn't rated all the movies, so if we're going to sum over this, we're going to sum over only over the values of i where r(i,j)=1. So we're going to sum only over the movies i that user j has actually rated. So that's what this denotes, sum of all values of i where r(i,j)=1. Meaning that user j has rated that movie i. And then finally we can take the usual normalization 1 over m(j). 

And this is very much like the cost function we have for linear regression with m or really m(j) training examples. Where you're summing over the m(j) movies for which you have a rating taking a squared error and the normalizing by this 1 over 2m(j). And this is going to be a cost function J of w(j), b(j). And if we minimize this as a function of w(j) and b(j), then you should come up with a pretty good choice of parameters w(i) and b(j). For making predictions for user j's ratings. Let me have just one more term to this cost function, which is the regularization term to prevent overfitting. And so here's our usual regularization parameter, lambda divided by 2m(j) and then times as sum of the squared values of the parameters w. And so n is a number of numbers in X(i) and that's the same as a number of numbers in w(j). If you were to minimize this cost function J as a function of w and b, you should get a pretty good set of parameters for predicting user j's ratings for other movies. Now, before moving on, it turns out that for recommended systems it would be convenient to actually eliminate this division by m(j) term, m(j) is just a constant in this expression. And so, even if you take it out, you should end up with the same value of w and b. 
Now let me take this cost function down here to the bottom and copy it to the next slide. So we have that to learn the parameters w(j), b(j) for user j. We would minimize this cost function as a function of w(j) and b(j). But instead of focusing on a single user, let's look at how we learn the parameters for all of the users. To learn the parameters w(1), b(1), w(2), b(2),...,w(nu), b(nu), we would take this cost function on top and sum it over all the nu users. So we would have sum from j=1 one to nu of the same cost function that we had written up above. And this becomes the cost for learning all the parameters for all of the users. And if we use gradient descent or any other optimization algorithm to minimize this as a function of w(1), b(1) all the way through w(nu), b(nu), then you have a pretty good set of parameters for predicting movie ratings for all the users. And you may notice that this algorithm is a lot like linear regression, where that plays a role similar to the output f(x) of linear regression. Only now we're training a different linear regression model for each of the nu users. So that's how you can learn parameters and predict movie ratings, if you had access to these features X1 and X2. That tell you how much is each of the movies, a romance movie, and how much is each of the movies an action movie? But where do these features come from? And what if you don't have access to such features that give you enough detail about the movies with wish to make these predictions? In the next video, we'll look at the modification of this algorithm. They'll let you make predictions that you make recommendations. Even if you don't have an advanced features that describe the items of the movies in sufficient detail to run the algorithm that we just saw. Let's go on and take a look at that in the next video

让我们来看看如果我们有每个物品的特征或每部电影的特征，我们如何开发一个推荐系统。这里是我们先前使用的相同数据集，其中四个用户对五部电影中的一些进行了评分，但并非全部。如果我们额外拥有电影的特征会怎样呢？因此，我添加了两个特征X1和X2，用于表示每个电影的浪漫程度和动作程度。例如，《最后的爱情》是一部非常浪漫的电影，因此该特征取值为0.9，但它完全不是一部动作片，所以该特征取值为0。但是《无尽的汽车追逐》中有一点浪漫成分，所以该特征取值为0.1，但它有很多动作成分，所以该特征取值为1.0。
你还记得我使用符号ν来表示用户的数量，这里是4，m表示电影的数量，这里是5。我还要介绍符号n，用来表示这里的特征数量。所以n=2，因为我们每部电影有两个特征X1和X2。有了这些特征，例如第一部电影《最后的爱情》的特征值为0.90。而第三部电影《可爱的爱情小狗》的特征值为0.99和0。让我们首先看一下如何对Alice的电影评分进行预测。对于用户一，也就是Alice，假设我们将电影i的评分预测为w.X(i)+b。这就像线性回归一样。例如，如果我们选择参数w(1)=[5,0]，b(1)=0，那么对于特征为0.99和0的第三部电影，预测值将是w.X(3)+b=0.99乘以5加上0乘以0，结果等于4.95。这个评分似乎是很合理的。
看起来Alice给《最后的爱情》和《永远的浪漫》这两部浪漫电影评分很高，但对动作片《无尽的汽车追逐》和《剑与空手道》评分很低。因此，如果我们看看《可爱的爱情小狗》这部电影，预测她可能给它评分为4.95似乎是相当合理的。因此，对于Alice来说，这些参数w和b似乎是预测她的电影评分的一个合理模型。我们需要稍微修改一下符号，因为我们不只有一个用户，而是多个用户，也就是说ν等于4个用户。我在这里加上上标1来表示这是用户1的参数w(1)，在那里也加上上标1。类似地，在这里和这里也是如此，所以我们实际上对数据集中的每个用户都有不同的参数。更一般地，在这个模型中，对于用户j，不仅仅是用户1，我们可以预测用户j对电影i的评分为w(j).X(i)+b(j)。所以这里的参数w(j)和b(j)是用于预测用户j对电影i的评分的参数，它是X(i)的函数，而X(i)是电影i的特征。这很像线性回归，只是我们为数据集中的每个用户拟合了不同的线性回归模型。那么让我们来看看如何为这个算法制定代价函数。提醒一下，符号上的说明是，如果用户j对电影i进行了评分，则r(i,j)=1，否则为0。而y(i,j)表示用户j对电影i的评分。
在前面的内容中，我们定义了w(j)和b(j)作为用户j的参数，X(i)作为电影i的特征向量。所以我们的模型是针对用户j和电影i，预测评分为w(j).X(i)+b(j)。我要引入一个新的符号，用m(j)表示用户j评分的电影数量。如果用户评分了4部电影，那么m(j)就等于4。如果用户评分了3部电影，那么m(j)就等于3。因此，我们希望根据我们拥有的数据来学习参数w(j)和b(j)，也就是给定用户对一组电影的评分。我们要使用的平均值非常类似于线性回归。所以让我们写出学习给定用户j的参数w(j)和b(j)的代价函数。现在我们只关注一个用户j。我要使用均方误差准则。所以代价将是预测值，即w(j).X(i)+b(j)，减去用户给出的实际评分，即y(i,j)，然后再平方。我们的目标是选择参数w和b，以最小化预测评分与实际观察到的评分之间的平方误差。但是用户并没有对所有电影进行评分，所以如果我们要进行求和，我们只会对r(i,j)=1的i值进行求和。我们只会对用户j实际评分的电影i进行求和。这就是这个符号的含义，对于r(i,j)=1的所有i值求和。意思是用户j对电影i进行了评分。最后，我们可以进行常规的归一化，即除以m(j)。
这与线性回归的代价函数非常相似，只是我们有m或者说m(j)个训练样本。你对有评分的m(j)部电影进行求和，计算平方误差，然后通过除以2m(j)进行归一化。这将成为一个关于w(j)和b(j)的代价函数J。如果我们将其作为w(j)和b(j)的函数进行最小化，你应该能够得到一个很好的选择，用于预测用户j的评分的参数w(i)和b(j)。现在，我要给这个代价函数添加一个额外的项，用于正则化以防止过拟合。所以这里是我们通常的正则化参数lambda除以2m(j)，然后乘以参数w的平方值的总和。n是X(i)和w(j)中的数字数量，与参数数量相同。如果你将这个代价函数J作为w和b的函数进行最小化，你应该能够得到一组很好的参数，用于预测用户j对其他电影的评分。在继续之前，事实证明对于推荐系统来说，消除这个除以m(j)的项会更方便，m(j)在这个表达式中只是一个常数。所以，即使去掉它，你应该得到相同的w和b的值。
让我将这个成本函数移到下方，并复制到下一页。因此，我们要学习用户j的参数w(j)和b(j)。我们将最小化这个成本函数，作为w(j)和b(j)的函数。但是，不仅仅关注单个用户，让我们看看如何学习所有用户的参数。为了学习w(1)，b(1)，w(2)，b(2)，...，w(nu)，b(nu)的参数，我们将以上面的成本函数为基础，在所有nu个用户上进行求和。因此，我们将从j=1到nu求和，使用与之前编写的相同的成本函数。这将成为学习所有用户参数的成本函数。如果我们使用梯度下降或任何其他优化算法，将其作为w(1)，b(1)一直到w(nu)，b(nu)的函数最小化，那么您将得到一个非常好的参数集，用于预测所有用户的电影评分。您可能会注意到，这个算法很像线性回归，其中f(x)的作用类似于线性回归的输出。只是现在，我们为每个用户训练了一个不同的线性回归模型。这就是如果您可以访问描述每部电影是浪漫电影和动作电影的特征X1和X2，您可以学习参数并预测电影评分的方式。但是这些特征从哪里来呢？如果您无法获得足够详细的特征来进行预测，又该怎么办呢？在下一个视频中，我们将讨论这个算法的修改，使您能够进行预测并进行推荐，即使没有足够详细的描述电影项目的高级特征来运行我们刚才看到的算法。让我们继续观看下一个视频，了解更多信息。